# ğŸ•·ï¸ Sitemap Crawler â€” Queue-Based Backend System

A production-ready, asynchronous sitemap crawler built using **Node.js, Express, BullMQ, Redis (Upstash), and MongoDB**.  
The system is designed to crawl large sitemaps safely using background workers, with full fault tolerance, retry handling, and link graph analysis.

---

## ğŸš€ Key Features

- ğŸ” **Queue-based crawling** (BullMQ + Redis)
- âš¡ **Non-blocking API** (instant response)
- ğŸ‘· **Background workers** with concurrency
- ğŸ”„ **Retry + exponential backoff**
- ğŸ§  **Idempotent design** (no duplicate crawling)
- ğŸ“Š **Crawl status tracking**
- ğŸ”— **Outgoing & Incoming link graph**
- ğŸ’¥ **Crash recovery tested**
- ğŸ“ˆ **Scalable & production-ready architecture**

---

## ğŸ—ï¸ Architecture Overview

Client
|
| POST /crawl/sitemap
v
API Server (Express)
|
| Enqueue Jobs
v
Redis (Upstash) + BullMQ Queue
|
| Consume Jobs
v
Worker Process
|
| Crawl HTML + Extract Links
v
MongoDB


- API never performs crawling
- Workers handle all heavy tasks
- Redis ensures job durability

---

## ğŸ“ Project Structure

src/
â”œâ”€â”€ api/
â”‚ â”œâ”€â”€ controllers/
â”‚ â”‚ â”œâ”€â”€ crawl.controller.js
â”‚ â”‚ â””â”€â”€ page.controller.js
â”‚ â”œâ”€â”€ routes/
â”‚ â”‚ â”œâ”€â”€ crawl.routes.js
â”‚ â”‚ â””â”€â”€ page.routes.js
â”‚ â””â”€â”€ server.js
â”‚
â”œâ”€â”€ worker/
â”‚ â”œâ”€â”€ crawl.worker.js
â”‚ â””â”€â”€ buildIncomingLinks.js
â”‚
â”œâ”€â”€ queue/
â”‚ â””â”€â”€ crawl.queue.js
â”‚
â”œâ”€â”€ models/
â”‚ â””â”€â”€ Page.model.js
â”‚
â”œâ”€â”€ config/
â”‚ â”œâ”€â”€ mongo.js
â”‚ â””â”€â”€ redis.js
â”‚
â””â”€â”€ .env


---

## ğŸ”§ Environment Setup

Create a `.env` file:

```env
PORT=5000
MONGO_URI=mongodb+srv://...
REDIS_URL=rediss://...
â–¶ï¸ How to Run
1ï¸âƒ£ Install dependencies
npm install
2ï¸âƒ£ Start API server
npm run dev
3ï¸âƒ£ Start worker (separate terminal)
node src/worker/crawl.worker.js
ğŸ”Œ API Endpoints
ğŸ”¹ Trigger Crawl
POST /crawl/sitemap
Fetches sitemap

Enqueues crawl jobs

Responds immediately (non-blocking)

ğŸ”¹ Crawl Status
GET /crawl/status
Response:

{
  "total": 100,
  "success": 98,
  "failed": 2,
  "pending": 0
}
ğŸ”¹ Outgoing Links
POST /pages/outgoing
Body: { "url": "https://example.com" }
ğŸ”¹ Incoming Links
POST /pages/incoming
Body: { "url": "https://example.com" }
ğŸ”¹ Top Linked Pages
GET /pages/top-linked?n=5
ğŸ”„ Crawl Lifecycle Explained
pending â†’ success
pending â†’ failed (after retries)
pending = queued / processing

success = crawled successfully

failed = unreachable after retries

ğŸ§  Idempotency & Re-Crawl Logic
Each job uses deterministic jobId = hash(url)

MongoDB enforces unique URLs

Already successful pages are skipped

Failed pages are retried on next run

if (page && page.crawlStatus === "success") {
  continue;
}
ğŸ§ª FULL TEST CHECKLIST (ALL EXECUTED âœ…)
âœ… Phase 0 â€” Clean State
MongoDB cleared

Redis flushed

âœ… Phase 1 â€” Health Check
API server runs

Worker connects to Redis & Mongo

âœ… Phase 2 â€” Non-Blocking API
/crawl/sitemap responds instantly

Crawling runs in background

âœ… Phase 3 â€” Status Tracking
pending, success, failed update correctly

Counts match real state

âœ… Phase 4 â€” Deduplication
Re-trigger does not duplicate data

Successful pages are skipped

Failed pages are retried

âœ… Phase 5 â€” Crash & Retry Test
Worker killed mid-crawl

Jobs persisted in Redis

Worker restart resumes jobs

Retries executed with backoff

âœ… Link Graph Validation
Outgoing links stored during crawl

Incoming links built post-crawl

Home page shows high incoming count (expected)

ğŸ§  Design Decisions
Queue over direct crawl â†’ scalability

Worker isolation â†’ fault tolerance

Post-crawl aggregation â†’ faster crawling

Selective reprocessing â†’ efficient retries

ğŸ¤ Interview-Ready Summary
â€œThe crawler is fully asynchronous and queue-driven. APIs enqueue jobs, workers process them with retries and backoff, and link graphs are computed as a post-crawl aggregation. The system is idempotent, fault-tolerant, and production-ready.â€